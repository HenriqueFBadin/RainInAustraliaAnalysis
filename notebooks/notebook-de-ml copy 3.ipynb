{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Projeto de Machine Learning </h1><center>\n",
    "<center><h1>Previsão de Chuvas na Austrália</h1><center>\n",
    "\n",
    "------------------------------------------------\n",
    "<center>Eduardo Selber, Henrique Badin e Luca Caruso<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T19:21:40.072080Z",
     "iopub.status.busy": "2024-11-03T19:21:40.071412Z",
     "iopub.status.idle": "2024-11-03T19:21:40.787204Z",
     "shell.execute_reply": "2024-11-03T19:21:40.786057Z",
     "shell.execute_reply.started": "2024-11-03T19:21:40.072022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importando as bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import sweetviz as sv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "#Carregando o dataset\n",
    "df = pd.read_csv(\"weatherAUS.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerando um relatorio de analise exploratoria\n",
    "#report = sv.analyze(df)\n",
    "#report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "Primeiro vamos formatar os dados da base de dados para prepará-lo para o modelo preditivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratamento de valores ausentes: Deletando as linhas com os valores ausentes das colunas \"Raintoday\" e \"RainTomorrow\"\n",
    "df = df.dropna(subset=[\"RainToday\", \"RainTomorrow\"])\n",
    "\n",
    "#Deixando apenas as colunas Date, Location, MinTemp, MaxTemp, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Temp9am, Temp3pm, RainToday, RainTomorrow\n",
    "df = df[[\"Date\", \"Location\", \"MinTemp\", \"MaxTemp\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Temp9am\", \"Temp3pm\", \"RainToday\", \"RainTomorrow\"]]\n",
    "df = df.dropna()\n",
    "\n",
    "# #Tratamento dos outliers e features engineering:\n",
    "#MinTemp\n",
    "df = df[(df[\"MinTemp\"] >= -10) & (df[\"MinTemp\"] <= 30)]\n",
    "\n",
    "#MaxTemp\n",
    "df = df[(df[\"MaxTemp\"] >= 5) & (df[\"MaxTemp\"] <= 40)]\n",
    "\n",
    "#Humidity9am não possui outliers\n",
    "#Humidity3pm não possui outliers\n",
    "\n",
    "#Pressure9am\n",
    "df = df[(df[\"Pressure9am\"] >= 1000) & (df[\"Pressure9am\"] <= 1035)]\n",
    "\n",
    "#Pressure3pm\n",
    "df = df[(df[\"Pressure3pm\"] >= 1000) & (df[\"Pressure3pm\"] <= 1035)]\n",
    "\n",
    "#Temp9am\n",
    "df = df[(df[\"Temp9am\"] >= 5) & (df[\"Temp9am\"] <= 30)]\n",
    "\n",
    "#Temp3pm\n",
    "df = df[(df[\"Temp3pm\"] >= 8) & (df[\"Temp3pm\"] <= 35)]\n",
    "\n",
    "#Tratamento de variaveis categoricas \n",
    "df[\"RainToday\"] = df[\"RainToday\"].map({\"Yes\":1, \"No\":0})\n",
    "df['RainTomorrow'] = df['RainTomorrow'].replace({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - Exploratory Data Analysis\n",
    "Primeiro vamos analisar os dados que são fornecidos pela base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colunas do dataset\n",
    "- **Date** - A data da observação (formato: yyyy-mm-dd)\n",
    "- **Location** - A localização da estação meteorológica (Nome da cidade - string)\n",
    "- **MinTemp** - A temperatura mínima em graus Celsius (float)\n",
    "- **MaxTemp** - A temperatura máxima em graus Celsius  (float)\n",
    "- **Rainfall** - A quantidade de chuva medida em mm (float)\n",
    "- **Evaporation** - A taxa de evaporação medida em mm (float)\n",
    "- **Sunshine** - O número de horas de sol (float)\n",
    "- **WindGustDir** - A direção da rajada de vento mais forte (string)\n",
    "- **WindGustSpeed** - A velocidade (em km/h) da rajada de vento mais forte (float)\n",
    "- **WindDir9am** - A direção do vento às 9am (string)\n",
    "- **WindDir3pm** - A direção do vento às 3pm (string)\n",
    "- **WindSpeed9am** - A velocidade do vento às 9am (em km/h) (float)\n",
    "- **WindSpeed3pm** - A velocidade do vento às 3pm (em km/h) (float)\n",
    "- **Humidity9am** - A umidade relativa às 9am (em %) (float)\n",
    "- **Humidity3pm** - A umidade relativa às 3pm (em %) (float)\n",
    "- **Pressure9am** - A pressão atmosférica reduzida ao nível do mar às 9am (em hpa) (float)\n",
    "- **Pressure3pm** - A pressão atmosférica reduzida ao nível do mar às 3pm (em hpa) (float)\n",
    "- **Cloud9am** - A fração de cobertura de nuvens às 9am (em oitavos) (float)\n",
    "- **Cloud3pm** - A fração de cobertura de nuvens às 3pm (em oitavos) (float)\n",
    "- **Temp9am** - A temperatura às 9am em graus Celsius (float)\n",
    "- **Temp3pm** - A temperatura às 3pm em graus Celsius (float)\n",
    "- **RainToday** - Se choveu (precipitação acima de 1mm) ou não (string)\n",
    "- **RainTomorrow** - A variável alvo. Se choverá ou não amanhã (string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as métricas do dataset\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas descritivas do DataFrame\n",
    "df.describe()\n",
    "\n",
    "print(\"VARIÁVEIS NUMÉRICAS\")\n",
    "# Distribuição das variáveis numéricas\n",
    "df.hist(bins=30, figsize=(20, 15), layout=(6, 4))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(\"VARIÁVEIS CATEGÓRICAS\")\n",
    "# # Contagem das variáveis categóricas do DataFrame\n",
    "# plt.figure(figsize=(20, 15))\n",
    "# for i, column in enumerate(df.select_dtypes(include='object').columns, 1):\n",
    "#     plt.subplot(3, 3, i)\n",
    "#     sns.countplot(data=df, y=column)\n",
    "#     plt.title(f'{column} Count')\n",
    "#     plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T19:21:40.789498Z",
     "iopub.status.busy": "2024-11-03T19:21:40.788959Z",
     "iopub.status.idle": "2024-11-03T19:21:40.799475Z",
     "shell.execute_reply": "2024-11-03T19:21:40.798319Z",
     "shell.execute_reply.started": "2024-11-03T19:21:40.789443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T19:21:40.803456Z",
     "iopub.status.busy": "2024-11-03T19:21:40.802655Z",
     "iopub.status.idle": "2024-11-03T19:21:40.857422Z",
     "shell.execute_reply": "2024-11-03T19:21:40.856262Z",
     "shell.execute_reply.started": "2024-11-03T19:21:40.803397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrando o dataset para a cidade de Sydney\n",
    "df = df[df[\"Location\"] == \"Sydney\"]\n",
    "\n",
    "#Ordenando o dataset pela coluna Date\n",
    "df = df.sort_values(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo o dataset em train test e validação\n",
    "#Treinamento: 60%\n",
    "#Teste: 20%\n",
    "#Validação: 20%\n",
    "\n",
    "#quantidade de linhas do dataset\n",
    "n_lines = df.shape[0]\n",
    "\n",
    "#separando o dataset em treinamento, teste e validação\n",
    "train = df[0:int(n_lines*0.6)]\n",
    "validation = df[int(n_lines*0.6):int(n_lines*0.8)]\n",
    "test = df[int(n_lines*0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_y(df):\n",
    "    df = df.drop([\"Date\", \"Location\"], axis=1)\n",
    "    x = df.drop(\"RainTomorrow\", axis=1).copy()\n",
    "    y = df[\"RainTomorrow\"].copy()\n",
    "    return x, y\n",
    "x_train, y_train = make_x_y(train)\n",
    "x_validation, y_validation = make_x_y(validation)\n",
    "x_test, y_test = make_x_y(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(x, y, columns_orig, window_size):\n",
    "    x_window = []\n",
    "    y_window = []\n",
    "    for i in range(len(x)-window_size-1):\n",
    "        x_window.append(x[i:i+window_size].reshape(-1))\n",
    "        y_window.append(y[i+window_size])\n",
    "\n",
    "    columns = []\n",
    "    for i in range(window_size):\n",
    "        for column in columns_orig:\n",
    "            columns.append(f\"{column}_{i}\")\n",
    "    return np.array(x_window), np.array(y_window), columns\n",
    "\n",
    "window = 7\n",
    "columns_orig = train.columns[2:-1]\n",
    "x_train, y_train, columns = make_windows(x_train.values, y_train.values, columns_orig, window)\n",
    "x_validation, y_validation, _ = make_windows(x_validation.values, y_validation.values, columns_orig, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_validation).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Função para construir o modelo Sequential\n",
    "def build_sequential_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=x_train.shape[1], activation=\"relu\"))  # Camada oculta\n",
    "    model.add(Dropout(0.3))  # Regularização\n",
    "    model.add(Dense(32, activation=\"relu\"))  # Outra camada oculta\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Saída binária\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Modelo Sequential como KerasClassifier\n",
    "keras_model = KerasClassifier(build_fn=build_sequential_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Reamostragem da classe minoritária\n",
    "ros = RandomOverSampler(sampling_strategy=\"minority\")\n",
    "x_resampled, y_resampled = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "# Adicionando modelos ao dicionário\n",
    "modelos = {\n",
    "    \"RandomForest_balanced\": RandomForestClassifier(n_estimators=1000, max_depth=10, class_weight=\"balanced\"),\n",
    "    \"GradientBoosting_balanced\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"LogisticRegression_balanced\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "    \"SVC_balanced\": SVC(kernel=\"linear\", class_weight=\"balanced\"),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Sequential_Keras\": keras_model\n",
    "}\n",
    "\n",
    "# Treinar e avaliar cada modelo\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"\\nModelo: {nome}\")\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", modelo)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(x_resampled, y_resampled)\n",
    "    y_pred_val = pipeline.predict(x_validation)\n",
    "    \n",
    "    # Métricas\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(confusion_matrix(y_validation, y_pred_val))\n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(y_validation, y_pred_val))\n",
    "    print(\"\\nAcurácia:\", accuracy_score(y_validation, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contagem de corretos e incorretos\n",
    "certo = 0\n",
    "total = 0\n",
    "for i in range(len(y_pred_val)):\n",
    "    print(y_validation[i], y_pred_val[i])\n",
    "    if y_validation[i] == y_pred_val[i]:\n",
    "        certo += 1\n",
    "    total += 1\n",
    "\n",
    "errado = total - certo\n",
    "\n",
    "# Dados para o gráfico\n",
    "labels = ['Correto', 'Incorreto']\n",
    "values = [certo, errado]\n",
    "\n",
    "# Criando o gráfico de barras\n",
    "plt.bar(labels, values, color=['green', 'red'])\n",
    "plt.xlabel('Previsões')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.title('Quantidade de Previsões Corretas e Incorretas')\n",
    "plt.show()\n",
    "print(f'Acurácia: {certo/total*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6012,
     "sourceId": 1733506,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
